import numpy as np
import torch

#é€ç‚¹å‰é¦ˆç¥ç»ç½‘ç»œ
class PointWiseFeedForward(torch.nn.Module):
    def __init__(self, hidden_units, dropout_rate):#æŒ‡å®šäº†å·ç§¯å±‚çš„è¾“å…¥å’Œè¾“å‡ºé€šé“æ•°ã€‚æŒ‡å®šäº†ä¸¢å¼ƒå±‚çš„ä¸¢å¼ƒç‡

        super(PointWiseFeedForward, self).__init__()

        self.conv1 = torch.nn.Conv1d(hidden_units, hidden_units, kernel_size=1)#è¿™äº›æ˜¯ 1D å·ç§¯å±‚ (torch.nn.Conv1d)ï¼Œå·ç§¯æ ¸å¤§å°è®¾ç½®ä¸º 1ï¼Œå¯¹è¾“å…¥æ•°æ®çš„æ¯ä¸ªä½ç½®ï¼ˆé€ç‚¹ï¼‰è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œå¯ç”¨äºæå–ç‰¹å¾æˆ–è€…èåˆä¿¡æ¯ç­‰
        self.dropout1 = torch.nn.Dropout(p=dropout_rate)#dropout1 å’Œ dropout2ï¼šä¸¢å¼ƒå±‚ï¼Œç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºç½®é›¶ä¸€äº›è¾“å…¥ï¼Œèµ·åˆ°é˜²æ­¢è¿‡æ‹Ÿåˆçš„ä½œç”¨ã€‚
        self.relu = torch.nn.ReLU()#reluï¼šReLU æ¿€æ´»å‡½æ•°ï¼Œåˆ†åˆ«åº”ç”¨äºæ¯ä¸ªå·ç§¯å±‚ä¹‹åï¼Œå¢åŠ éçº¿æ€§
        self.conv2 = torch.nn.Conv1d(hidden_units, hidden_units, kernel_size=1)
        self.dropout2 = torch.nn.Dropout(p=dropout_rate)

    def forward(self, inputs):#inputsï¼ˆå¼ é‡ï¼‰ï¼šå¯èƒ½å½¢çŠ¶æ˜¯(batch_size, sequence_length, hidden_units)
        #torch.nn.Conv1déœ€è¦è¾“å…¥å½¢çŠ¶ä¸º(batch_size, channels, length)   é€šè¿‡inputs.transpose(-1, -2)å°†å…¶ç»´åº¦é¡ºåºè°ƒæ•´ä¸ºé€‚åˆ 1 ç»´å·ç§¯å±‚è¾“å…¥çš„æ ¼å¼
        outputs = self.dropout2(self.conv2(self.relu(self.dropout1(self.conv1(inputs.transpose(-1, -2))))))
        outputs = outputs.transpose(-1, -2) # as Conv1D requires (N, C, Length)
        outputs += inputs#æ®‹å·®è¿æ¥
        return outputs
        # è¾“å…¥å¼ é‡é¦–å…ˆé€šè¿‡ conv1ï¼Œå¯¹åºåˆ—è¿›è¡Œ 1D å·ç§¯ã€‚
        # dropout1 åº”ç”¨äº conv1 çš„è¾“å‡ºï¼Œä»¥éšæœºä¸¢å¼ƒä¸€äº›å…ƒç´ ã€‚
        # ç»“æœé€šè¿‡ relu æ¿€æ´»å‡½æ•°å¢åŠ éçº¿æ€§ã€‚
        # ç»“æœå†ç»è¿‡ conv2ï¼Œè¿›è¡Œå¦ä¸€è½® 1D å·ç§¯ã€‚
        # dropout2 åº”ç”¨äº conv2 çš„è¾“å‡ºã€‚
        # æœ€ç»ˆè¾“å‡ºé€šè¿‡è½¬ç½®æœ€åä¸¤ä¸ªç»´åº¦æ¢å¤åˆ°åŸæ¥çš„ç»´åº¦ï¼Œä»¥ä¾¿é€‚åº”åç»­æ¨¡å‹ä¸­çš„è¾“å…¥æ ¼å¼ã€‚
        # åŸå§‹è¾“å…¥ (inputs) ç„¶åè¢«æ·»åŠ åˆ°è¾“å‡ºä¸Šï¼Œä»¥å®ç°æ®‹å·®è¿æ¥ï¼Œè¿™æœ‰åŠ©äºè®­ç»ƒæ›´æ·±çš„ç½‘ç»œï¼Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹æ›´åŠ é«˜æ•ˆå’Œç¨³å®šã€‚æ®‹å·®è¿æ¥åœ¨å¤„ç†æ·±åº¦ç¥ç»ç½‘ç»œæ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå®ƒé€šè¿‡ä¿æŒæ¢¯åº¦æµåŠ¨çš„ç¨³å®šæ€§æ¥é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚
    #åŸå§‹å¼ é‡--äº¤æ¢ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªç»´åº¦-ã€‹--ä¸€ç»´å·ç§¯-->

# pls use the following self-made multihead attention layer
# in case your pytorch version is below 1.16 or for other reasons
# https://github.com/pmixer/TiSASRec.pytorch/blob/master/model.py

#åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„åºåˆ—æ¨èæ¨¡å‹SASRec
class SASRec(torch.nn.Module):
    def __init__(self, user_num, item_num, args):
        super(SASRec, self).__init__()

        self.user_num = user_num
        self.item_num = item_num
        self.dev = args.device

        # TODO: loss += args.l2_emb for regularizing embedding vectors during training
        # https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch
        self.item_emb = torch.nn.Embedding(self.item_num+1, args.hidden_units, padding_idx=0)#item_embåµŒå…¥å±‚ï¼Œç”¨äºå°†ç‰©å“ç¼–å·ï¼ˆä»0åˆ°item_numï¼‰æ˜ å°„åˆ°ç»´åº¦ä¸ºargs.hidden_unitsçš„å‘é‡ç©ºé—´
        self.pos_emb = torch.nn.Embedding(args.maxlen+1, args.hidden_units, padding_idx=0)#pos_embæ˜¯ä½ç½®åµŒå…¥å±‚ï¼Œç”¨äºç»™åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®èµ‹äºˆä¸€ä¸ªç‰¹å®šçš„åµŒå…¥å‘é‡ï¼ŒåŒæ ·ç»´åº¦æ˜¯args.hidden_units
        self.emb_dropout = torch.nn.Dropout(p=args.dropout_rate)#å®šä¹‰äº†emb_dropoutå±‚ï¼Œç”¨äºåœ¨åµŒå…¥å±‚è¾“å‡ºåä»¥æ¦‚ç‡args.dropout_rateè¿›è¡Œéšæœºå¤±æ´»æ“ä½œï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

        #æ„å»ºæ¨¡å—åˆ—è¡¨  åˆ†åˆ«åˆ›å»ºäº†å››ä¸ªModuleListç±»å‹çš„åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨åç»­æ„å»ºçš„å¤šä¸ªå±‚å½’ä¸€åŒ–ã€å¤šå¤´æ³¨æ„åŠ›ä»¥åŠé€ç‚¹å‰é¦ˆç½‘ç»œå±‚
        #torch.nn.ModuleList()æ˜¯ PyTorch ä¸­çš„ä¸€ä¸ªç±»ï¼Œå®ƒæ˜¯ä¸€ä¸ªå­˜å‚¨æ¨¡å—ï¼ˆModuleï¼‰çš„åˆ—è¡¨ã€‚åœ¨ç¥ç»ç½‘ç»œçš„æ„å»ºä¸­ï¼Œå®ƒç”¨äºæ–¹ä¾¿åœ°ç®¡ç†ä¸€ç³»åˆ—çš„ç¥ç»ç½‘ç»œå±‚æˆ–å…¶ä»–è‡ªå®šä¹‰æ¨¡å—ã€‚
        self.attention_layernorms = torch.nn.ModuleList() # to be Q for self-attention
        self.attention_layers = torch.nn.ModuleList()
        self.forward_layernorms = torch.nn.ModuleList()
        self.forward_layers = torch.nn.ModuleList()
        #è¿˜å®šä¹‰äº†last_layernormï¼Œè¿™æ˜¯æœ€åä¸€å±‚çš„å±‚å½’ä¸€åŒ–å±‚ï¼Œç”¨äºå¯¹ç»è¿‡å¤šå±‚å¤„ç†åçš„è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–æ“ä½œï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹å’Œæå‡æ¨¡å‹æ€§èƒ½
        self.last_layernorm = torch.nn.LayerNorm(args.hidden_units, eps=1e-8)

        #å¾ªç¯æ„å»ºå„å±‚ã€‚
        for _ in range(args.num_blocks):
            #æ¯æ¬¡å¾ªç¯åˆ›å»ºä¸€ä¸ªæ–°çš„å±‚å½’ä¸€åŒ–å¯¹è±¡new_attn_layernormå¹¶æ·»åŠ åˆ°attention_layernormsåˆ—è¡¨ä¸­ï¼Œç”¨äºå¯¹å¤šå¤´æ³¨æ„åŠ›å±‚çš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚
            new_attn_layernorm = torch.nn.LayerNorm(args.hidden_units, eps=1e-8)
            self.attention_layernorms.append(new_attn_layernorm)

            #åˆ›å»ºä¸€ä¸ªtorch.nn.MultiheadAttentionå¯¹è±¡ï¼Œå¹¶æ·»åŠ åˆ°attention_layersåˆ—è¡¨ä¸­
            new_attn_layer =  torch.nn.MultiheadAttention(args.hidden_units,
                                                            args.num_heads,
                                                            args.dropout_rate)
            self.attention_layers.append(new_attn_layer)

            # åˆ›å»ºä¸€ä¸ªæ–°çš„å±‚å½’ä¸€åŒ–å¯¹è±¡new_fwd_layernormæ·»åŠ åˆ°forward_layernormsåˆ—è¡¨ï¼Œç”¨äºå¯¹é€ç‚¹å‰é¦ˆç½‘ç»œçš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ã€‚
            new_fwd_layernorm = torch.nn.LayerNorm(args.hidden_units, eps=1e-8)
            self.forward_layernorms.append(new_fwd_layernorm)

            #åˆ›å»ºä¸€ä¸ªPointWiseFeedForwardç±»çš„å®ä¾‹new_fwd_layerï¼Œå¹¶æ·»åŠ åˆ°forward_layersåˆ—è¡¨ä¸­ï¼Œé€ç‚¹å‰é¦ˆç½‘ç»œç”¨äºå¯¹ç»è¿‡æ³¨æ„åŠ›æœºåˆ¶å¤„ç†åçš„ç‰¹å¾è¿›è¡Œè¿›ä¸€æ­¥çš„éçº¿æ€§å˜æ¢å’Œç‰¹å¾èåˆç­‰æ“ä½œã€‚
            new_fwd_layer = PointWiseFeedForward(args.hidden_units, args.dropout_rate)
            self.forward_layers.append(new_fwd_layer)

            # self.pos_sigmoid = torch.nn.Sigmoid()
            # self.neg_sigmoid = torch.nn.Sigmoid()


    #é¦–å…ˆæ¥æ”¶log_seqsä½œä¸ºè¾“å…¥ï¼Œå®ƒåº”è¯¥æ˜¯è¡¨ç¤ºç”¨æˆ·è¡Œä¸ºåºåˆ—
    def log2feats(self, log_seqs): # TODO: fp64 and int64 as default in python, trim?
        #                  è½¬æ¢ä¸º PyTorch çš„é•¿æ•´å‹å¼ é‡ğŸ‘‡  ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ğŸ‘‡     self.item_embé€šè¿‡self.item_embåµŒå…¥å±‚å°†ç‰©å“ç¼–å·è½¬æ¢ä¸ºå¯¹åº”çš„åµŒå…¥å‘é‡ï¼Œå¾—åˆ°seqs
        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.dev))
        seqs *= self.item_emb.embedding_dim ** 0.5  #è¿™å¯èƒ½æ˜¯ä¸€ç§å¸¸è§çš„åˆå§‹åŒ–æˆ–è€…ç¼©æ”¾æ“ä½œï¼Œæœ‰åŠ©äºæ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ•ˆæœä¼˜åŒ–
        #ç”Ÿæˆä½ç½®ä¿¡æ¯å¼ é‡poss   å¤åˆ¶ä»1åˆ°åºåˆ—é•¿åº¦çš„æ•°ç»„æ¥åŒ¹é…è¾“å…¥åºåˆ—çš„æ‰¹é‡ç»´åº¦
        poss = np.tile(np.arange(1, log_seqs.shape[1] + 1), [log_seqs.shape[0], 1])
        # TODO: directly do tensor = torch.arange(1, xxx, device='cuda') to save extra overheads
        #å±è”½æ— æ•ˆçš„ç‰©å“ä¿¡æ¯
        poss *= (log_seqs != 0)
        #å°†ä½ç½®ä¿¡æ¯æ·»åŠ åˆ°seqsä¸Šã€‚
        seqs += self.pos_emb(torch.LongTensor(poss).to(self.dev))
        seqs = self.emb_dropout(seqs)

        tl = seqs.shape[1] # time dim len for enforce causality
        attention_mask = ~torch.tril(torch.ones((tl, tl), dtype=torch.bool, device=self.dev))

        for i in range(len(self.attention_layers)):
            seqs = torch.transpose(seqs, 0, 1)
            Q = self.attention_layernorms[i](seqs)
            mha_outputs, _ = self.attention_layers[i](Q, seqs, seqs, 
                                            attn_mask=attention_mask)
                                            # need_weights=False) this arg do not work?
            seqs = Q + mha_outputs
            seqs = torch.transpose(seqs, 0, 1)

            seqs = self.forward_layernorms[i](seqs)
            seqs = self.forward_layers[i](seqs)

        log_feats = self.last_layernorm(seqs) # (U, T, C) -> (U, -1, C)

        return log_feats

    #é€šè¿‡ç”¨æˆ·çš„å†å²è¡Œä¸ºåºåˆ—ï¼Œè®¡ç®—å‡ºæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„å¾—åˆ†ï¼ˆlogitsï¼‰ï¼Œå¹¶ä¸ºåç»­çš„æŸå¤±å‡½æ•°è®¡ç®—æä¾›è¾“å…¥ã€‚
    def forward(self, user_ids, log_seqs, pos_seqs, neg_seqs): # for training        
        log_feats = self.log2feats(log_seqs) # user_ids hasn't been used yet

        pos_embs = self.item_emb(torch.LongTensor(pos_seqs).to(self.dev))
        neg_embs = self.item_emb(torch.LongTensor(neg_seqs).to(self.dev))

        pos_logits = (log_feats * pos_embs).sum(dim=-1)
        neg_logits = (log_feats * neg_embs).sum(dim=-1)

        # pos_pred = self.pos_sigmoid(pos_logits)
        # neg_pred = self.neg_sigmoid(neg_logits)

        return pos_logits, neg_logits # pos_pred, neg_pred

    #æ ¹æ®ç”¨æˆ·çš„å†å²è¡Œä¸ºåºåˆ—æ¥é¢„æµ‹ç”¨æˆ·å¯¹ä¸åŒç‰©å“çš„å…´è¶£æˆ–åå¥½ï¼Œå¹¶è¾“å‡ºæ¯ä¸ªç‰©å“çš„å¾—åˆ†ï¼ˆlogitsï¼‰ã€‚
    def predict(self, user_ids, log_seqs, item_indices): # for inference
        log_feats = self.log2feats(log_seqs) # user_ids hasn't been used yet
        #è¿™é‡Œä» log_feats ä¸­æå–åºåˆ—çš„æœ€åä¸€æ—¶åˆ»ï¼ˆå³æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼‰å¯¹åº”çš„ç‰¹å¾ã€‚
        final_feat = log_feats[:, -1, :] # only use last QKV classifier, a waste

        item_embs = self.item_emb(torch.LongTensor(item_indices).to(self.dev)) # (U, I, C)

        logits = item_embs.matmul(final_feat.unsqueeze(-1)).squeeze(-1)

        # preds = self.pos_sigmoid(logits) # rank same item list for different users

        return logits # preds # (U, I)
